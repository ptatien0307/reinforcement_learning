{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ve4yxCI7Oqpy",
        "PwaAsHr97tkX",
        "hy1D7-OUb9I_",
        "KPJ_uOF3b8Py"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "ve4yxCI7Oqpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.25.2\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender==1.0.2\n",
        "!pip install \"gym[atari, accept-rom-license]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxI9Ws5olXnk",
        "outputId": "3c0aab1b-af3b-4a22-d621-75025aee2c5f",
        "execution": {
          "iopub.status.busy": "2023-07-08T10:19:04.999231Z",
          "iopub.execute_input": "2023-07-08T10:19:04.999674Z",
          "iopub.status.idle": "2023-07-08T10:19:51.504294Z",
          "shell.execute_reply.started": "2023-07-08T10:19:04.999638Z",
          "shell.execute_reply": "2023-07-08T10:19:51.503023Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: gym==0.25.2 in /opt/conda/lib/python3.10/site-packages (0.25.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (1.23.5)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (0.0.8)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: colabgymrender==1.0.2 in /opt/conda/lib/python3.10/site-packages (1.0.2)\nRequirement already satisfied: pyvirtualdisplay in /opt/conda/lib/python3.10/site-packages (from colabgymrender==1.0.2) (3.0)\nRequirement already satisfied: moviepy in /opt/conda/lib/python3.10/site-packages (from colabgymrender==1.0.2) (1.0.3)\nRequirement already satisfied: gym in /opt/conda/lib/python3.10/site-packages (from colabgymrender==1.0.2) (0.25.2)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from colabgymrender==1.0.2) (4.7.0.72)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym->colabgymrender==1.0.2) (1.23.5)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym->colabgymrender==1.0.2) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym->colabgymrender==1.0.2) (0.0.8)\nRequirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from moviepy->colabgymrender==1.0.2) (4.4.2)\nRequirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy->colabgymrender==1.0.2) (4.64.1)\nRequirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy->colabgymrender==1.0.2) (2.28.2)\nRequirement already satisfied: proglog<=1.0.0 in /opt/conda/lib/python3.10/site-packages (from moviepy->colabgymrender==1.0.2) (0.1.10)\nRequirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy->colabgymrender==1.0.2) (2.28.1)\nRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from moviepy->colabgymrender==1.0.2) (0.4.8)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender==1.0.2) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: gym[accept-rom-license,atari] in /opt/conda/lib/python3.10/site-packages (0.25.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (1.23.5)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (0.0.8)\nRequirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (0.4.2)\nRequirement already satisfied: ale-py~=0.7.5 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (0.7.5)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.12.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.28.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\nRequirement already satisfied: AutoROM.accept-rom-license in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "_zltNqshPkMv",
        "execution": {
          "iopub.status.busy": "2023-07-08T10:19:51.507688Z",
          "iopub.execute_input": "2023-07-08T10:19:51.508079Z",
          "iopub.status.idle": "2023-07-08T10:19:51.515655Z",
          "shell.execute_reply.started": "2023-07-08T10:19:51.508044Z",
          "shell.execute_reply": "2023-07-08T10:19:51.514666Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "vSOX6U--CQGs",
        "execution": {
          "iopub.status.busy": "2023-07-08T10:19:51.516703Z",
          "iopub.execute_input": "2023-07-08T10:19:51.517063Z",
          "iopub.status.idle": "2023-07-08T10:19:51.526651Z",
          "shell.execute_reply.started": "2023-07-08T10:19:51.517028Z",
          "shell.execute_reply": "2023-07-08T10:19:51.525575Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normal Replay Memory"
      ],
      "metadata": {
        "id": "PwaAsHr97tkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalReplayMemory():\n",
        "    def __init__(self, max_size, min_replay_size):\n",
        "        self.max_size = max_size\n",
        "        self.memory = deque(maxlen=self.max_size)\n",
        "        self.min_replay_size = min_replay_size\n",
        "\n",
        "    def add(self, experience):\n",
        "        state, action, reward, done, next_state = experience\n",
        "        state = preprocess(state) # resize and convert to gray scale\n",
        "        next_state = preprocess(next_state) # resize and convert to gray scale\n",
        "        experience = (state, action, reward, done, next_state)\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        experiences = random.sample(self.memory, batch_size)\n",
        "\n",
        "        # Take batches from experiences\n",
        "        states = np.array([experience[0] for experience in experiences])\n",
        "        actions = np.array([experience[1] for experience in experiences])\n",
        "        rewards = np.array([experience[2] for experience in experiences])\n",
        "        dones = np.array([experience[3] for experience in experiences])\n",
        "        next_states = np.array([experience[4] for experience in experiences])\n",
        "\n",
        "        # Convert to tensor\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64).to(device).unsqueeze(-1) # (batch_size,) --> (batch_size, 1)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device).unsqueeze(-1) # (batch_size,) --> (batch_size, 1)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(device).unsqueeze(-1) # (batch_size,) --> (batch_size, 1)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "\n",
        "        return states, actions, rewards, dones, next_states\n"
      ],
      "metadata": {
        "id": "jZMXK6gi7zOm",
        "execution": {
          "iopub.status.busy": "2023-07-08T10:19:51.529509Z",
          "iopub.execute_input": "2023-07-08T10:19:51.530656Z",
          "iopub.status.idle": "2023-07-08T10:19:51.557317Z",
          "shell.execute_reply.started": "2023-07-08T10:19:51.530622Z",
          "shell.execute_reply": "2023-07-08T10:19:51.556432Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "hy1D7-OUb9I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(state):\n",
        "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
        "    state_resize = cv2.resize(state, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
        "    state_resize = np.reshape(state_resize, (1, 84, 84))\n",
        "\n",
        "    return state_resize"
      ],
      "metadata": {
        "id": "-2YSrs5otWMI",
        "execution": {
          "iopub.status.busy": "2023-07-08T10:19:51.560207Z",
          "iopub.execute_input": "2023-07-08T10:19:51.560866Z",
          "iopub.status.idle": "2023-07-08T10:19:51.574980Z",
          "shell.execute_reply.started": "2023-07-08T10:19:51.560834Z",
          "shell.execute_reply": "2023-07-08T10:19:51.573709Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "KPJ_uOF3b8Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuronNetwork(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(NeuronNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Extraction\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Prediction\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=11552, out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=1024, out_features=env.action_space.n)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.network(state)\n",
        "        return x\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # Compute max q value\n",
        "        state = preprocess(state)\n",
        "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "        q_values = self(state.unsqueeze(0)) # pytorch requires inputs in terms of batch\n",
        "        best_action = torch.argmax(q_values, dim=1)[0]\n",
        "\n",
        "        return best_action.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "knPFtLbsl1R4",
        "execution": {
          "iopub.status.busy": "2023-07-08T10:19:51.577397Z",
          "iopub.execute_input": "2023-07-08T10:19:51.578300Z",
          "iopub.status.idle": "2023-07-08T10:19:51.588445Z",
          "shell.execute_reply.started": "2023-07-08T10:19:51.578269Z",
          "shell.execute_reply": "2023-07-08T10:19:51.587353Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ],
      "metadata": {
        "id": "QrqeZiKd0FQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, max_epsilon, min_epsilon, max_num_steps, \\\n",
        "                 epsilon_decay_intervals, gamma, alpha, \\\n",
        "                 memory_size, min_replay_size, batch_size, \\\n",
        "                 target_update_frequency):\n",
        "        # Environment\n",
        "        self.env = env\n",
        "        self.memory = NormalReplayMemory(max_size=memory_size, min_replay_size=min_replay_size)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.max_num_steps = max_num_steps\n",
        "        self.epsilon_decay_intervals = epsilon_decay_intervals\n",
        "        self.gamma = gamma # discount value\n",
        "        self.alpha = alpha # learning rate\n",
        "        self.batch_size = batch_size # batch size taken from memory\n",
        "        self.target_update_frequency = target_update_frequency # target network update frequency\n",
        "\n",
        "        # Network\n",
        "        self.q_net = NeuronNetwork(self.env).to(device)\n",
        "        self.target_net = NeuronNetwork(self.env).to(device)\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=self.alpha)\n",
        "\n",
        "    def choose_action(self, epsilon, state):\n",
        "        random_number = np.random.uniform(0,1)\n",
        "        if random_number <= epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.q_net.choose_action(state)\n",
        "        return action\n",
        "\n",
        "    def fill_memory(self):\n",
        "        state = self.env.reset()\n",
        "\n",
        "        # Loop min_replay_size times and append experience to memory\n",
        "        for _ in range(self.memory.min_replay_size):\n",
        "\n",
        "            # Randomly taking action\n",
        "            action = self.env.action_space.sample()\n",
        "\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            experience = (state, action, reward, done, next_state)\n",
        "\n",
        "            # Add to memory\n",
        "            self.memory.add(experience)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "\n",
        "    def training(self):\n",
        "        # Fill memory\n",
        "        self.fill_memory()\n",
        "        reward_buffer = deque(maxlen=100) # Rewards of the previous 100 episodes\n",
        "\n",
        "        reward_per_episode = 0.0\n",
        "        state = self.env.reset()\n",
        "        all_rewards = []\n",
        "\n",
        "        for step in range(self.max_num_steps):\n",
        "            # Computer epsilon\n",
        "            epsilon = np.interp(step, [0, self.epsilon_decay_intervals], [self.max_epsilon, self.min_epsilon])\n",
        "            # Choose action to take\n",
        "            action = self.choose_action(epsilon, state)\n",
        "\n",
        "            # Take action and add experience to memory\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            experience = (state, action, reward, done, next_state)\n",
        "            # Add to memory\n",
        "            self.memory.add(experience)\n",
        "\n",
        "            reward_per_episode += reward\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            # If done, 1 episode is done\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                reward_buffer.append(reward_per_episode)\n",
        "                all_rewards.append((step, reward_per_episode))\n",
        "                reward_per_episode = 0.0\n",
        "\n",
        "            # Get batch from memory for training\n",
        "            states, actions, rewards, dones, next_states = self.memory.get_batch(self.batch_size)\n",
        "\n",
        "            # Predict Q value with Q network\n",
        "            q_values = self.q_net(states)\n",
        "            action_q_values = torch.gather(input=q_values, dim=1, index=actions)\n",
        "\n",
        "            # Predict target with Target network\n",
        "            # Compute targets using the formulation sample = r + gamma * max q(s',a')\n",
        "            a = self.q_net(next_states).argmax(dim=1, keepdim=True)\n",
        "            max_target_q_values = self.target_net(next_states).gather(1, a).detach()\n",
        "            targets = rewards + self.gamma * (1 - dones) * max_target_q_values\n",
        "\n",
        "            loss = torch.nn.functional.mse_loss(action_q_values, targets)\n",
        "\n",
        "            # Gradient descent for q-network\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Update target network\n",
        "            if (step + 1) % self.target_update_frequency == 0:\n",
        "                self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "            # Print training results\n",
        "            if (step + 1) % 1000 == 0:\n",
        "                average_reward = np.mean(reward_buffer)\n",
        "                print(f'Episode: {len(all_rewards)} Step: {step+1} Average reward: {average_reward}')\n",
        "        return all_rewards"
      ],
      "metadata": {
        "id": "nAQTIfAVIKwE",
        "execution": {
          "iopub.status.busy": "2023-07-08T10:19:51.589941Z",
          "iopub.execute_input": "2023-07-08T10:19:51.590508Z",
          "iopub.status.idle": "2023-07-08T10:19:51.617984Z",
          "shell.execute_reply.started": "2023-07-08T10:19:51.590471Z",
          "shell.execute_reply": "2023-07-08T10:19:51.617091Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "max_num_steps = 500000\n",
        "epsilon_decay_intervals = 200000\n",
        "gamma = 0.99 # discount\n",
        "alpha = 5e-4 # learning rate\n",
        "\n",
        "memory_size = 50000\n",
        "min_replay_size = 1000\n",
        "batch_size = 32\n",
        "\n",
        "target_update_frequency = 2000"
      ],
      "metadata": {
        "id": "WB0ZRTihvREN",
        "execution": {
          "iopub.status.busy": "2023-07-08T12:38:44.878155Z",
          "iopub.execute_input": "2023-07-08T12:38:44.878567Z",
          "iopub.status.idle": "2023-07-08T12:38:44.884231Z",
          "shell.execute_reply.started": "2023-07-08T12:38:44.878535Z",
          "shell.execute_reply": "2023-07-08T12:38:44.883284Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"ALE/KungFuMaster-v5\")\n",
        "model = Agent(env, max_epsilon, min_epsilon, max_num_steps, \\\n",
        "                 epsilon_decay_intervals, gamma, alpha, \\\n",
        "                 memory_size, min_replay_size, batch_size, \\\n",
        "                 target_update_frequency)\n",
        "all_rewards = model.training()"
      ],
      "metadata": {
        "id": "x2M07vUo7BKU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63adf4cd-d906-4397-f8bf-8e76c86f2c70",
        "execution": {
          "iopub.status.busy": "2023-07-08T12:38:46.842860Z",
          "iopub.execute_input": "2023-07-08T12:38:46.843740Z",
          "iopub.status.idle": "2023-07-08T14:20:00.180575Z",
          "shell.execute_reply.started": "2023-07-08T12:38:46.843698Z",
          "shell.execute_reply": "2023-07-08T14:20:00.179480Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Episode: 0 Step: 1000 Average reward: nan\nEpisode: 1 Step: 2000 Average reward: 600.0\nEpisode: 3 Step: 3000 Average reward: 400.0\nEpisode: 4 Step: 4000 Average reward: 375.0\nEpisode: 5 Step: 5000 Average reward: 460.0\nEpisode: 5 Step: 6000 Average reward: 460.0\nEpisode: 6 Step: 7000 Average reward: 550.0\nEpisode: 8 Step: 8000 Average reward: 512.5\nEpisode: 9 Step: 9000 Average reward: 488.8888888888889\nEpisode: 10 Step: 10000 Average reward: 440.0\nEpisode: 11 Step: 11000 Average reward: 427.27272727272725\nEpisode: 12 Step: 12000 Average reward: 425.0\nEpisode: 13 Step: 13000 Average reward: 476.9230769230769\nEpisode: 13 Step: 14000 Average reward: 476.9230769230769\nEpisode: 15 Step: 15000 Average reward: 473.3333333333333\nEpisode: 16 Step: 16000 Average reward: 462.5\nEpisode: 17 Step: 17000 Average reward: 476.47058823529414\nEpisode: 18 Step: 18000 Average reward: 488.8888888888889\nEpisode: 19 Step: 19000 Average reward: 505.2631578947368\nEpisode: 19 Step: 20000 Average reward: 505.2631578947368\nEpisode: 21 Step: 21000 Average reward: 504.76190476190476\nEpisode: 22 Step: 22000 Average reward: 527.2727272727273\nEpisode: 23 Step: 23000 Average reward: 521.7391304347826\nEpisode: 24 Step: 24000 Average reward: 545.8333333333334\nEpisode: 25 Step: 25000 Average reward: 576.0\nEpisode: 26 Step: 26000 Average reward: 588.4615384615385\nEpisode: 27 Step: 27000 Average reward: 592.5925925925926\nEpisode: 27 Step: 28000 Average reward: 592.5925925925926\nEpisode: 29 Step: 29000 Average reward: 582.7586206896551\nEpisode: 30 Step: 30000 Average reward: 583.3333333333334\nEpisode: 31 Step: 31000 Average reward: 577.4193548387096\nEpisode: 31 Step: 32000 Average reward: 577.4193548387096\nEpisode: 32 Step: 33000 Average reward: 609.375\nEpisode: 33 Step: 34000 Average reward: 624.2424242424242\nEpisode: 34 Step: 35000 Average reward: 614.7058823529412\nEpisode: 35 Step: 36000 Average reward: 611.4285714285714\nEpisode: 36 Step: 37000 Average reward: 613.8888888888889\nEpisode: 37 Step: 38000 Average reward: 610.8108108108108\nEpisode: 39 Step: 39000 Average reward: 602.5641025641025\nEpisode: 39 Step: 40000 Average reward: 602.5641025641025\nEpisode: 40 Step: 41000 Average reward: 602.5\nEpisode: 41 Step: 42000 Average reward: 617.0731707317074\nEpisode: 42 Step: 43000 Average reward: 616.6666666666666\nEpisode: 43 Step: 44000 Average reward: 637.2093023255813\nEpisode: 44 Step: 45000 Average reward: 640.9090909090909\nEpisode: 45 Step: 46000 Average reward: 668.8888888888889\nEpisode: 46 Step: 47000 Average reward: 682.6086956521739\nEpisode: 47 Step: 48000 Average reward: 687.2340425531914\nEpisode: 48 Step: 49000 Average reward: 679.1666666666666\nEpisode: 49 Step: 50000 Average reward: 675.5102040816327\nEpisode: 50 Step: 51000 Average reward: 668.0\nEpisode: 50 Step: 52000 Average reward: 668.0\nEpisode: 52 Step: 53000 Average reward: 717.3076923076923\nEpisode: 53 Step: 54000 Average reward: 705.6603773584906\nEpisode: 54 Step: 55000 Average reward: 711.1111111111111\nEpisode: 55 Step: 56000 Average reward: 700.0\nEpisode: 56 Step: 57000 Average reward: 694.6428571428571\nEpisode: 57 Step: 58000 Average reward: 700.0\nEpisode: 58 Step: 59000 Average reward: 693.1034482758621\nEpisode: 59 Step: 60000 Average reward: 698.3050847457627\nEpisode: 60 Step: 61000 Average reward: 696.6666666666666\nEpisode: 60 Step: 62000 Average reward: 696.6666666666666\nEpisode: 61 Step: 63000 Average reward: 700.0\nEpisode: 62 Step: 64000 Average reward: 696.7741935483871\nEpisode: 63 Step: 65000 Average reward: 700.0\nEpisode: 64 Step: 66000 Average reward: 704.6875\nEpisode: 65 Step: 67000 Average reward: 700.0\nEpisode: 66 Step: 68000 Average reward: 695.4545454545455\nEpisode: 67 Step: 69000 Average reward: 694.0298507462686\nEpisode: 68 Step: 70000 Average reward: 700.0\nEpisode: 69 Step: 71000 Average reward: 701.4492753623189\nEpisode: 70 Step: 72000 Average reward: 704.2857142857143\nEpisode: 71 Step: 73000 Average reward: 704.2253521126761\nEpisode: 72 Step: 74000 Average reward: 715.2777777777778\nEpisode: 73 Step: 75000 Average reward: 720.5479452054794\nEpisode: 74 Step: 76000 Average reward: 733.7837837837837\nEpisode: 75 Step: 77000 Average reward: 726.6666666666666\nEpisode: 76 Step: 78000 Average reward: 728.9473684210526\nEpisode: 77 Step: 79000 Average reward: 735.0649350649351\nEpisode: 78 Step: 80000 Average reward: 737.1794871794872\nEpisode: 78 Step: 81000 Average reward: 737.1794871794872\nEpisode: 80 Step: 82000 Average reward: 743.75\nEpisode: 80 Step: 83000 Average reward: 743.75\nEpisode: 81 Step: 84000 Average reward: 765.4320987654321\nEpisode: 82 Step: 85000 Average reward: 764.6341463414634\nEpisode: 83 Step: 86000 Average reward: 766.2650602409639\nEpisode: 84 Step: 87000 Average reward: 758.3333333333334\nEpisode: 85 Step: 88000 Average reward: 761.1764705882352\nEpisode: 86 Step: 89000 Average reward: 758.1395348837209\nEpisode: 87 Step: 90000 Average reward: 757.471264367816\nEpisode: 88 Step: 91000 Average reward: 757.9545454545455\nEpisode: 89 Step: 92000 Average reward: 767.4157303370787\nEpisode: 90 Step: 93000 Average reward: 772.2222222222222\nEpisode: 91 Step: 94000 Average reward: 779.1208791208791\nEpisode: 92 Step: 95000 Average reward: 781.5217391304348\nEpisode: 93 Step: 96000 Average reward: 780.6451612903226\nEpisode: 94 Step: 97000 Average reward: 785.1063829787234\nEpisode: 95 Step: 98000 Average reward: 784.2105263157895\nEpisode: 96 Step: 99000 Average reward: 782.2916666666666\nEpisode: 97 Step: 100000 Average reward: 790.7216494845361\nEpisode: 98 Step: 101000 Average reward: 792.8571428571429\nEpisode: 99 Step: 102000 Average reward: 800.0\nEpisode: 100 Step: 103000 Average reward: 799.0\nEpisode: 101 Step: 104000 Average reward: 804.0\nEpisode: 102 Step: 105000 Average reward: 801.0\nEpisode: 103 Step: 106000 Average reward: 802.0\nEpisode: 104 Step: 107000 Average reward: 805.0\nEpisode: 105 Step: 108000 Average reward: 814.0\nEpisode: 106 Step: 109000 Average reward: 811.0\nEpisode: 107 Step: 110000 Average reward: 812.0\nEpisode: 108 Step: 111000 Average reward: 812.0\nEpisode: 110 Step: 112000 Average reward: 826.0\nEpisode: 111 Step: 113000 Average reward: 830.0\nEpisode: 112 Step: 114000 Average reward: 842.0\nEpisode: 113 Step: 115000 Average reward: 842.0\nEpisode: 114 Step: 116000 Average reward: 837.0\nEpisode: 115 Step: 117000 Average reward: 847.0\nEpisode: 115 Step: 118000 Average reward: 847.0\nEpisode: 116 Step: 119000 Average reward: 865.0\nEpisode: 117 Step: 120000 Average reward: 861.0\nEpisode: 119 Step: 121000 Average reward: 852.0\nEpisode: 120 Step: 122000 Average reward: 860.0\nEpisode: 121 Step: 123000 Average reward: 864.0\nEpisode: 122 Step: 124000 Average reward: 859.0\nEpisode: 123 Step: 125000 Average reward: 863.0\nEpisode: 123 Step: 126000 Average reward: 863.0\nEpisode: 124 Step: 127000 Average reward: 858.0\nEpisode: 125 Step: 128000 Average reward: 863.0\nEpisode: 126 Step: 129000 Average reward: 865.0\nEpisode: 127 Step: 130000 Average reward: 866.0\nEpisode: 128 Step: 131000 Average reward: 871.0\nEpisode: 129 Step: 132000 Average reward: 874.0\nEpisode: 130 Step: 133000 Average reward: 896.0\nEpisode: 131 Step: 134000 Average reward: 904.0\nEpisode: 132 Step: 135000 Average reward: 895.0\nEpisode: 133 Step: 136000 Average reward: 896.0\nEpisode: 134 Step: 137000 Average reward: 908.0\nEpisode: 135 Step: 138000 Average reward: 906.0\nEpisode: 136 Step: 139000 Average reward: 910.0\nEpisode: 138 Step: 140000 Average reward: 927.0\nEpisode: 139 Step: 141000 Average reward: 930.0\nEpisode: 140 Step: 142000 Average reward: 930.0\nEpisode: 141 Step: 143000 Average reward: 932.0\nEpisode: 142 Step: 144000 Average reward: 933.0\nEpisode: 143 Step: 145000 Average reward: 921.0\nEpisode: 144 Step: 146000 Average reward: 922.0\nEpisode: 145 Step: 147000 Average reward: 912.0\nEpisode: 146 Step: 148000 Average reward: 906.0\nEpisode: 148 Step: 149000 Average reward: 906.0\nEpisode: 149 Step: 150000 Average reward: 909.0\nEpisode: 150 Step: 151000 Average reward: 910.0\nEpisode: 151 Step: 152000 Average reward: 886.0\nEpisode: 152 Step: 153000 Average reward: 897.0\nEpisode: 153 Step: 154000 Average reward: 903.0\nEpisode: 154 Step: 155000 Average reward: 907.0\nEpisode: 156 Step: 156000 Average reward: 920.0\nEpisode: 157 Step: 157000 Average reward: 912.0\nEpisode: 158 Step: 158000 Average reward: 935.0\nEpisode: 159 Step: 159000 Average reward: 937.0\nEpisode: 160 Step: 160000 Average reward: 937.0\nEpisode: 161 Step: 161000 Average reward: 931.0\nEpisode: 162 Step: 162000 Average reward: 942.0\nEpisode: 163 Step: 163000 Average reward: 941.0\nEpisode: 164 Step: 164000 Average reward: 938.0\nEpisode: 165 Step: 165000 Average reward: 942.0\nEpisode: 166 Step: 166000 Average reward: 946.0\nEpisode: 167 Step: 167000 Average reward: 952.0\nEpisode: 169 Step: 168000 Average reward: 940.0\nEpisode: 170 Step: 169000 Average reward: 933.0\nEpisode: 171 Step: 170000 Average reward: 934.0\nEpisode: 172 Step: 171000 Average reward: 925.0\nEpisode: 173 Step: 172000 Average reward: 924.0\nEpisode: 174 Step: 173000 Average reward: 933.0\nEpisode: 175 Step: 174000 Average reward: 947.0\nEpisode: 177 Step: 175000 Average reward: 941.0\nEpisode: 178 Step: 176000 Average reward: 938.0\nEpisode: 179 Step: 177000 Average reward: 938.0\nEpisode: 180 Step: 178000 Average reward: 949.0\nEpisode: 181 Step: 179000 Average reward: 936.0\nEpisode: 182 Step: 180000 Average reward: 943.0\nEpisode: 183 Step: 181000 Average reward: 936.0\nEpisode: 184 Step: 182000 Average reward: 939.0\nEpisode: 186 Step: 183000 Average reward: 940.0\nEpisode: 187 Step: 184000 Average reward: 937.0\nEpisode: 188 Step: 185000 Average reward: 943.0\nEpisode: 189 Step: 186000 Average reward: 933.0\nEpisode: 190 Step: 187000 Average reward: 925.0\nEpisode: 192 Step: 188000 Average reward: 931.0\nEpisode: 192 Step: 189000 Average reward: 931.0\nEpisode: 193 Step: 190000 Average reward: 950.0\nEpisode: 194 Step: 191000 Average reward: 952.0\nEpisode: 195 Step: 192000 Average reward: 960.0\nEpisode: 196 Step: 193000 Average reward: 977.0\nEpisode: 197 Step: 194000 Average reward: 975.0\nEpisode: 198 Step: 195000 Average reward: 985.0\nEpisode: 199 Step: 196000 Average reward: 988.0\nEpisode: 200 Step: 197000 Average reward: 1005.0\nEpisode: 201 Step: 198000 Average reward: 1002.0\nEpisode: 202 Step: 199000 Average reward: 1035.0\nEpisode: 203 Step: 200000 Average reward: 1043.0\nEpisode: 204 Step: 201000 Average reward: 1063.0\nEpisode: 205 Step: 202000 Average reward: 1067.0\nEpisode: 206 Step: 203000 Average reward: 1062.0\nEpisode: 208 Step: 204000 Average reward: 1081.0\nEpisode: 209 Step: 205000 Average reward: 1073.0\nEpisode: 210 Step: 206000 Average reward: 1068.0\nEpisode: 211 Step: 207000 Average reward: 1073.0\nEpisode: 213 Step: 208000 Average reward: 1056.0\nEpisode: 214 Step: 209000 Average reward: 1058.0\nEpisode: 215 Step: 210000 Average reward: 1050.0\nEpisode: 216 Step: 211000 Average reward: 1040.0\nEpisode: 217 Step: 212000 Average reward: 1041.0\nEpisode: 218 Step: 213000 Average reward: 1057.0\nEpisode: 219 Step: 214000 Average reward: 1073.0\nEpisode: 220 Step: 215000 Average reward: 1071.0\nEpisode: 221 Step: 216000 Average reward: 1067.0\nEpisode: 222 Step: 217000 Average reward: 1106.0\nEpisode: 223 Step: 218000 Average reward: 1102.0\nEpisode: 224 Step: 219000 Average reward: 1123.0\nEpisode: 225 Step: 220000 Average reward: 1113.0\nEpisode: 226 Step: 221000 Average reward: 1106.0\nEpisode: 227 Step: 222000 Average reward: 1106.0\nEpisode: 228 Step: 223000 Average reward: 1101.0\nEpisode: 230 Step: 224000 Average reward: 1089.0\nEpisode: 230 Step: 225000 Average reward: 1089.0\nEpisode: 232 Step: 226000 Average reward: 1100.0\nEpisode: 233 Step: 227000 Average reward: 1107.0\nEpisode: 234 Step: 228000 Average reward: 1106.0\nEpisode: 235 Step: 229000 Average reward: 1124.0\nEpisode: 236 Step: 230000 Average reward: 1139.0\nEpisode: 237 Step: 231000 Average reward: 1137.0\nEpisode: 238 Step: 232000 Average reward: 1130.0\nEpisode: 239 Step: 233000 Average reward: 1148.0\nEpisode: 240 Step: 234000 Average reward: 1144.0\nEpisode: 241 Step: 235000 Average reward: 1153.0\nEpisode: 242 Step: 236000 Average reward: 1164.0\nEpisode: 243 Step: 237000 Average reward: 1189.0\nEpisode: 244 Step: 238000 Average reward: 1206.0\nEpisode: 245 Step: 239000 Average reward: 1234.0\nEpisode: 246 Step: 240000 Average reward: 1253.0\nEpisode: 247 Step: 241000 Average reward: 1261.0\nEpisode: 248 Step: 242000 Average reward: 1270.0\nEpisode: 249 Step: 243000 Average reward: 1293.0\nEpisode: 250 Step: 244000 Average reward: 1312.0\nEpisode: 251 Step: 245000 Average reward: 1315.0\nEpisode: 252 Step: 246000 Average reward: 1316.0\nEpisode: 253 Step: 247000 Average reward: 1336.0\nEpisode: 254 Step: 248000 Average reward: 1336.0\nEpisode: 255 Step: 249000 Average reward: 1355.0\nEpisode: 256 Step: 250000 Average reward: 1371.0\nEpisode: 257 Step: 251000 Average reward: 1374.0\nEpisode: 258 Step: 252000 Average reward: 1356.0\nEpisode: 259 Step: 253000 Average reward: 1365.0\nEpisode: 260 Step: 254000 Average reward: 1381.0\nEpisode: 261 Step: 255000 Average reward: 1398.0\nEpisode: 262 Step: 256000 Average reward: 1411.0\nEpisode: 262 Step: 257000 Average reward: 1411.0\nEpisode: 263 Step: 258000 Average reward: 1417.0\nEpisode: 264 Step: 259000 Average reward: 1430.0\nEpisode: 266 Step: 260000 Average reward: 1446.0\nEpisode: 267 Step: 261000 Average reward: 1460.0\nEpisode: 267 Step: 262000 Average reward: 1460.0\nEpisode: 269 Step: 263000 Average reward: 1499.0\nEpisode: 270 Step: 264000 Average reward: 1517.0\nEpisode: 271 Step: 265000 Average reward: 1529.0\nEpisode: 272 Step: 266000 Average reward: 1538.0\nEpisode: 273 Step: 267000 Average reward: 1545.0\nEpisode: 274 Step: 268000 Average reward: 1533.0\nEpisode: 275 Step: 269000 Average reward: 1553.0\nEpisode: 276 Step: 270000 Average reward: 1575.0\nEpisode: 276 Step: 271000 Average reward: 1575.0\nEpisode: 277 Step: 272000 Average reward: 1597.0\nEpisode: 278 Step: 273000 Average reward: 1645.0\nEpisode: 279 Step: 274000 Average reward: 1662.0\nEpisode: 280 Step: 275000 Average reward: 1697.0\nEpisode: 280 Step: 276000 Average reward: 1697.0\nEpisode: 281 Step: 277000 Average reward: 1755.0\nEpisode: 282 Step: 278000 Average reward: 1757.0\nEpisode: 283 Step: 279000 Average reward: 1773.0\nEpisode: 284 Step: 280000 Average reward: 1794.0\nEpisode: 285 Step: 281000 Average reward: 1821.0\nEpisode: 286 Step: 282000 Average reward: 1839.0\nEpisode: 287 Step: 283000 Average reward: 1868.0\nEpisode: 287 Step: 284000 Average reward: 1868.0\nEpisode: 288 Step: 285000 Average reward: 1892.0\nEpisode: 289 Step: 286000 Average reward: 1939.0\nEpisode: 290 Step: 287000 Average reward: 1959.0\nEpisode: 291 Step: 288000 Average reward: 1969.0\nEpisode: 292 Step: 289000 Average reward: 1986.0\nEpisode: 293 Step: 290000 Average reward: 1985.0\nEpisode: 294 Step: 291000 Average reward: 1998.0\nEpisode: 295 Step: 292000 Average reward: 2018.0\nEpisode: 296 Step: 293000 Average reward: 2015.0\nEpisode: 297 Step: 294000 Average reward: 2042.0\nEpisode: 298 Step: 295000 Average reward: 2052.0\nEpisode: 299 Step: 296000 Average reward: 2044.0\nEpisode: 300 Step: 297000 Average reward: 2040.0\nEpisode: 301 Step: 298000 Average reward: 2048.0\nEpisode: 302 Step: 299000 Average reward: 2052.0\nEpisode: 303 Step: 300000 Average reward: 2067.0\nEpisode: 304 Step: 301000 Average reward: 2076.0\nEpisode: 305 Step: 302000 Average reward: 2066.0\nEpisode: 306 Step: 303000 Average reward: 2082.0\nEpisode: 307 Step: 304000 Average reward: 2144.0\nEpisode: 307 Step: 305000 Average reward: 2144.0\nEpisode: 308 Step: 306000 Average reward: 2168.0\nEpisode: 309 Step: 307000 Average reward: 2195.0\nEpisode: 310 Step: 308000 Average reward: 2227.0\nEpisode: 311 Step: 309000 Average reward: 2256.0\nEpisode: 312 Step: 310000 Average reward: 2276.0\nEpisode: 313 Step: 311000 Average reward: 2314.0\nEpisode: 313 Step: 312000 Average reward: 2314.0\nEpisode: 314 Step: 313000 Average reward: 2353.0\nEpisode: 315 Step: 314000 Average reward: 2423.0\nEpisode: 316 Step: 315000 Average reward: 2441.0\nEpisode: 316 Step: 316000 Average reward: 2441.0\nEpisode: 317 Step: 317000 Average reward: 2468.0\nEpisode: 318 Step: 318000 Average reward: 2490.0\nEpisode: 319 Step: 319000 Average reward: 2581.0\nEpisode: 319 Step: 320000 Average reward: 2581.0\nEpisode: 320 Step: 321000 Average reward: 2631.0\nEpisode: 321 Step: 322000 Average reward: 2676.0\nEpisode: 322 Step: 323000 Average reward: 2733.0\nEpisode: 323 Step: 324000 Average reward: 2763.0\nEpisode: 323 Step: 325000 Average reward: 2763.0\nEpisode: 324 Step: 326000 Average reward: 2812.0\nEpisode: 325 Step: 327000 Average reward: 2848.0\nEpisode: 325 Step: 328000 Average reward: 2848.0\nEpisode: 326 Step: 329000 Average reward: 2928.0\nEpisode: 327 Step: 330000 Average reward: 2962.0\nEpisode: 328 Step: 331000 Average reward: 2968.0\nEpisode: 328 Step: 332000 Average reward: 2968.0\nEpisode: 329 Step: 333000 Average reward: 3045.0\nEpisode: 330 Step: 334000 Average reward: 3128.0\nEpisode: 331 Step: 335000 Average reward: 3166.0\nEpisode: 331 Step: 336000 Average reward: 3166.0\nEpisode: 332 Step: 337000 Average reward: 3222.0\nEpisode: 333 Step: 338000 Average reward: 3257.0\nEpisode: 334 Step: 339000 Average reward: 3283.0\nEpisode: 334 Step: 340000 Average reward: 3283.0\nEpisode: 335 Step: 341000 Average reward: 3346.0\nEpisode: 336 Step: 342000 Average reward: 3361.0\nEpisode: 337 Step: 343000 Average reward: 3387.0\nEpisode: 338 Step: 344000 Average reward: 3437.0\nEpisode: 339 Step: 345000 Average reward: 3458.0\nEpisode: 339 Step: 346000 Average reward: 3458.0\nEpisode: 340 Step: 347000 Average reward: 3524.0\nEpisode: 341 Step: 348000 Average reward: 3568.0\nEpisode: 341 Step: 349000 Average reward: 3568.0\nEpisode: 342 Step: 350000 Average reward: 3642.0\nEpisode: 343 Step: 351000 Average reward: 3674.0\nEpisode: 343 Step: 352000 Average reward: 3674.0\nEpisode: 344 Step: 353000 Average reward: 3764.0\nEpisode: 345 Step: 354000 Average reward: 3823.0\nEpisode: 346 Step: 355000 Average reward: 3825.0\nEpisode: 347 Step: 356000 Average reward: 3881.0\nEpisode: 347 Step: 357000 Average reward: 3881.0\nEpisode: 348 Step: 358000 Average reward: 3957.0\nEpisode: 349 Step: 359000 Average reward: 3986.0\nEpisode: 349 Step: 360000 Average reward: 3986.0\nEpisode: 350 Step: 361000 Average reward: 4002.0\nEpisode: 351 Step: 362000 Average reward: 4051.0\nEpisode: 352 Step: 363000 Average reward: 4110.0\nEpisode: 353 Step: 364000 Average reward: 4139.0\nEpisode: 354 Step: 365000 Average reward: 4157.0\nEpisode: 354 Step: 366000 Average reward: 4157.0\nEpisode: 355 Step: 367000 Average reward: 4202.0\nEpisode: 356 Step: 368000 Average reward: 4216.0\nEpisode: 356 Step: 369000 Average reward: 4216.0\nEpisode: 357 Step: 370000 Average reward: 4340.0\nEpisode: 358 Step: 371000 Average reward: 4395.0\nEpisode: 358 Step: 372000 Average reward: 4395.0\nEpisode: 359 Step: 373000 Average reward: 4432.0\nEpisode: 360 Step: 374000 Average reward: 4481.0\nEpisode: 360 Step: 375000 Average reward: 4481.0\nEpisode: 361 Step: 376000 Average reward: 4623.0\nEpisode: 362 Step: 377000 Average reward: 4628.0\nEpisode: 362 Step: 378000 Average reward: 4628.0\nEpisode: 363 Step: 379000 Average reward: 4702.0\nEpisode: 364 Step: 380000 Average reward: 4736.0\nEpisode: 365 Step: 381000 Average reward: 4780.0\nEpisode: 365 Step: 382000 Average reward: 4780.0\nEpisode: 366 Step: 383000 Average reward: 4824.0\nEpisode: 367 Step: 384000 Average reward: 4830.0\nEpisode: 368 Step: 385000 Average reward: 4876.0\nEpisode: 369 Step: 386000 Average reward: 4920.0\nEpisode: 369 Step: 387000 Average reward: 4920.0\nEpisode: 370 Step: 388000 Average reward: 4967.0\nEpisode: 371 Step: 389000 Average reward: 4993.0\nEpisode: 372 Step: 390000 Average reward: 5054.0\nEpisode: 373 Step: 391000 Average reward: 5041.0\nEpisode: 374 Step: 392000 Average reward: 5122.0\nEpisode: 374 Step: 393000 Average reward: 5122.0\nEpisode: 375 Step: 394000 Average reward: 5206.0\nEpisode: 376 Step: 395000 Average reward: 5194.0\nEpisode: 377 Step: 396000 Average reward: 5193.0\nEpisode: 377 Step: 397000 Average reward: 5193.0\nEpisode: 378 Step: 398000 Average reward: 5209.0\nEpisode: 379 Step: 399000 Average reward: 5248.0\nEpisode: 380 Step: 400000 Average reward: 5277.0\nEpisode: 381 Step: 401000 Average reward: 5229.0\nEpisode: 381 Step: 402000 Average reward: 5229.0\nEpisode: 382 Step: 403000 Average reward: 5277.0\nEpisode: 383 Step: 404000 Average reward: 5347.0\nEpisode: 383 Step: 405000 Average reward: 5347.0\nEpisode: 384 Step: 406000 Average reward: 5402.0\nEpisode: 385 Step: 407000 Average reward: 5437.0\nEpisode: 386 Step: 408000 Average reward: 5468.0\nEpisode: 386 Step: 409000 Average reward: 5468.0\nEpisode: 388 Step: 410000 Average reward: 5483.0\nEpisode: 389 Step: 411000 Average reward: 5432.0\nEpisode: 390 Step: 412000 Average reward: 5461.0\nEpisode: 391 Step: 413000 Average reward: 5458.0\nEpisode: 391 Step: 414000 Average reward: 5458.0\nEpisode: 392 Step: 415000 Average reward: 5487.0\nEpisode: 393 Step: 416000 Average reward: 5535.0\nEpisode: 394 Step: 417000 Average reward: 5573.0\nEpisode: 394 Step: 418000 Average reward: 5573.0\nEpisode: 395 Step: 419000 Average reward: 5588.0\nEpisode: 396 Step: 420000 Average reward: 5676.0\nEpisode: 396 Step: 421000 Average reward: 5676.0\nEpisode: 397 Step: 422000 Average reward: 5708.0\nEpisode: 398 Step: 423000 Average reward: 5720.0\nEpisode: 399 Step: 424000 Average reward: 5810.0\nEpisode: 400 Step: 425000 Average reward: 5802.0\nEpisode: 401 Step: 426000 Average reward: 5813.0\nEpisode: 402 Step: 427000 Average reward: 5790.0\nEpisode: 403 Step: 428000 Average reward: 5817.0\nEpisode: 403 Step: 429000 Average reward: 5817.0\nEpisode: 404 Step: 430000 Average reward: 5903.0\nEpisode: 405 Step: 431000 Average reward: 5940.0\nEpisode: 405 Step: 432000 Average reward: 5940.0\nEpisode: 406 Step: 433000 Average reward: 6005.0\nEpisode: 406 Step: 434000 Average reward: 6005.0\nEpisode: 407 Step: 435000 Average reward: 6039.0\nEpisode: 408 Step: 436000 Average reward: 6024.0\nEpisode: 409 Step: 437000 Average reward: 6029.0\nEpisode: 410 Step: 438000 Average reward: 6063.0\nEpisode: 410 Step: 439000 Average reward: 6063.0\nEpisode: 411 Step: 440000 Average reward: 6105.0\nEpisode: 411 Step: 441000 Average reward: 6105.0\nEpisode: 412 Step: 442000 Average reward: 6193.0\nEpisode: 412 Step: 443000 Average reward: 6193.0\nEpisode: 413 Step: 444000 Average reward: 6274.0\nEpisode: 414 Step: 445000 Average reward: 6327.0\nEpisode: 415 Step: 446000 Average reward: 6311.0\nEpisode: 416 Step: 447000 Average reward: 6308.0\nEpisode: 416 Step: 448000 Average reward: 6308.0\nEpisode: 417 Step: 449000 Average reward: 6319.0\nEpisode: 418 Step: 450000 Average reward: 6317.0\nEpisode: 419 Step: 451000 Average reward: 6264.0\nEpisode: 420 Step: 452000 Average reward: 6266.0\nEpisode: 421 Step: 453000 Average reward: 6255.0\nEpisode: 421 Step: 454000 Average reward: 6255.0\nEpisode: 422 Step: 455000 Average reward: 6268.0\nEpisode: 422 Step: 456000 Average reward: 6268.0\nEpisode: 423 Step: 457000 Average reward: 6318.0\nEpisode: 424 Step: 458000 Average reward: 6334.0\nEpisode: 424 Step: 459000 Average reward: 6334.0\nEpisode: 425 Step: 460000 Average reward: 6374.0\nEpisode: 426 Step: 461000 Average reward: 6354.0\nEpisode: 427 Step: 462000 Average reward: 6358.0\nEpisode: 427 Step: 463000 Average reward: 6358.0\nEpisode: 428 Step: 464000 Average reward: 6422.0\nEpisode: 429 Step: 465000 Average reward: 6400.0\nEpisode: 429 Step: 466000 Average reward: 6400.0\nEpisode: 430 Step: 467000 Average reward: 6346.0\nEpisode: 430 Step: 468000 Average reward: 6346.0\nEpisode: 431 Step: 469000 Average reward: 6462.0\nEpisode: 432 Step: 470000 Average reward: 6452.0\nEpisode: 432 Step: 471000 Average reward: 6452.0\nEpisode: 433 Step: 472000 Average reward: 6512.0\nEpisode: 434 Step: 473000 Average reward: 6515.0\nEpisode: 435 Step: 474000 Average reward: 6459.0\nEpisode: 436 Step: 475000 Average reward: 6440.0\nEpisode: 437 Step: 476000 Average reward: 6480.0\nEpisode: 437 Step: 477000 Average reward: 6480.0\nEpisode: 438 Step: 478000 Average reward: 6488.0\nEpisode: 439 Step: 479000 Average reward: 6531.0\nEpisode: 439 Step: 480000 Average reward: 6531.0\nEpisode: 440 Step: 481000 Average reward: 6513.0\nEpisode: 441 Step: 482000 Average reward: 6524.0\nEpisode: 441 Step: 483000 Average reward: 6524.0\nEpisode: 442 Step: 484000 Average reward: 6494.0\nEpisode: 443 Step: 485000 Average reward: 6494.0\nEpisode: 443 Step: 486000 Average reward: 6494.0\nEpisode: 444 Step: 487000 Average reward: 6454.0\nEpisode: 445 Step: 488000 Average reward: 6423.0\nEpisode: 446 Step: 489000 Average reward: 6463.0\nEpisode: 446 Step: 490000 Average reward: 6463.0\nEpisode: 447 Step: 491000 Average reward: 6441.0\nEpisode: 448 Step: 492000 Average reward: 6382.0\nEpisode: 449 Step: 493000 Average reward: 6402.0\nEpisode: 450 Step: 494000 Average reward: 6407.0\nEpisode: 450 Step: 495000 Average reward: 6407.0\nEpisode: 451 Step: 496000 Average reward: 6415.0\nEpisode: 451 Step: 497000 Average reward: 6415.0\nEpisode: 452 Step: 498000 Average reward: 6436.0\nEpisode: 453 Step: 499000 Average reward: 6464.0\nEpisode: 454 Step: 500000 Average reward: 6516.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.save('DDQN_all_rewards_500k',all_rewards)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-08T14:24:44.202100Z",
          "iopub.execute_input": "2023-07-08T14:24:44.202401Z",
          "iopub.status.idle": "2023-07-08T14:24:44.211444Z",
          "shell.execute_reply.started": "2023-07-08T14:24:44.202376Z",
          "shell.execute_reply": "2023-07-08T14:24:44.210471Z"
        },
        "trusted": true,
        "id": "NNzZbb9atD0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'DDQN_all_rewards_500k.npy')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-08T14:24:48.905967Z",
          "iopub.execute_input": "2023-07-08T14:24:48.906353Z",
          "iopub.status.idle": "2023-07-08T14:24:48.913151Z",
          "shell.execute_reply.started": "2023-07-08T14:24:48.906322Z",
          "shell.execute_reply": "2023-07-08T14:24:48.912122Z"
        },
        "trusted": true,
        "id": "QwA1y-s5tD0l",
        "outputId": "67f5d906-0d29-46c8-cb13-1acbb2259b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "/kaggle/working/ddqn_500.npy",
            "text/html": "<a href='ddqn_500.npy' target='_blank'>ddqn_500.npy</a><br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.q_net.state_dict(), \"./DDQN_all_rewards_500k\")"
      ],
      "metadata": {
        "id": "OC_H9zdWtg9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}