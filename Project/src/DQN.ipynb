{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ve4yxCI7Oqpy",
        "PwaAsHr97tkX",
        "hy1D7-OUb9I_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve4yxCI7Oqpy"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxI9Ws5olXnk",
        "outputId": "66e8e156-5dcb-48fe-c6df-40239b782cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colabgymrender==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (from colabgymrender==1.0.2) (3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender==1.0.2) (1.0.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from colabgymrender==1.0.2) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from colabgymrender==1.0.2) (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->colabgymrender==1.0.2) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->colabgymrender==1.0.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->colabgymrender==1.0.2) (0.0.8)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (2.27.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender==1.0.2) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (3.4)\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.65.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender==1.0.2\n",
        "!pip install \"gym[atari, accept-rom-license]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zltNqshPkMv",
        "outputId": "8e4a7993-722f-4b6f-927c-25ea375eb435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please use `sobel` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSOX6U--CQGs",
        "outputId": "f81a3abe-d55e-4972-aeba-5c769d072e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normal Replay Memory"
      ],
      "metadata": {
        "id": "PwaAsHr97tkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalReplayMemory():\n",
        "    def __init__(self, max_size, min_replay_size):\n",
        "        self.max_size = max_size\n",
        "        self.memory = deque(maxlen=self.max_size)\n",
        "        self.min_replay_size = min_replay_size\n",
        "\n",
        "    def add(self, experience):\n",
        "        state, action, reward, done, next_state = experience\n",
        "        state = preprocess(state) # resize and convert to gray scale\n",
        "        next_state = preprocess(next_state) # resize and convert to gray scale\n",
        "        experience = (state, action, reward, done, next_state)\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        experiences = random.sample(self.memory, batch_size)\n",
        "\n",
        "        # Take batches from experiences\n",
        "        states = np.array([experience[0] for experience in experiences])\n",
        "        actions = np.array([experience[1] for experience in experiences])\n",
        "        rewards = np.array([experience[2] for experience in experiences])\n",
        "        dones = np.array([experience[3] for experience in experiences])\n",
        "        next_states = np.array([experience[4] for experience in experiences])\n",
        "\n",
        "        # Convert to tensor\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64).to(device).unsqueeze(-1) # (batch_size,) --> (batch_size, 1)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device).unsqueeze(-1) # (batch_size,) --> (batch_size, 1)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(device).unsqueeze(-1) # (batch_size,) --> (batch_size, 1)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "\n",
        "        return states, actions, rewards, dones, next_states\n"
      ],
      "metadata": {
        "id": "jZMXK6gi7zOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy1D7-OUb9I_"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2YSrs5otWMI"
      },
      "outputs": [],
      "source": [
        "def preprocess(state):\n",
        "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
        "    state_resize = cv2.resize(state, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
        "    state_resize = np.reshape(state_resize, (1, 84, 84))\n",
        "\n",
        "    return state_resize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPJ_uOF3b8Py"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knPFtLbsl1R4"
      },
      "outputs": [],
      "source": [
        "class NeuronNetwork(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(NeuronNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Extraction\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Prediction\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=11552, out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=1024, out_features=env.action_space.n)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.network(state)\n",
        "        return x\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # Compute max q value\n",
        "        state = preprocess(state)\n",
        "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "        q_values = self(state.unsqueeze(0)) # pytorch requires inputs in terms of batch\n",
        "        best_action = torch.argmax(q_values, dim=1)[0]\n",
        "\n",
        "        return best_action.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrqeZiKd0FQO"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAQTIfAVIKwE"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, max_epsilon, min_epsilon, max_num_steps, \\\n",
        "                 epsilon_decay_intervals, gamma, alpha, \\\n",
        "                 memory_size, min_replay_size, batch_size, \\\n",
        "                 target_update_frequency):\n",
        "        # Environment\n",
        "        self.env = env\n",
        "        self.memory = NormalReplayMemory(max_size=memory_size, min_replay_size=min_replay_size)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.max_num_steps = max_num_steps\n",
        "        self.epsilon_decay_intervals = epsilon_decay_intervals\n",
        "        self.gamma = gamma # discount value\n",
        "        self.alpha = alpha # learning rate\n",
        "        self.batch_size = batch_size # batch size taken from memory\n",
        "        self.target_update_frequency = target_update_frequency # target network update frequency\n",
        "\n",
        "        # Network\n",
        "        self.q_net = NeuronNetwork(self.env).to(device)\n",
        "        self.target_net = NeuronNetwork(self.env).to(device)\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=self.alpha)\n",
        "\n",
        "    def choose_action(self, epsilon, state):\n",
        "        random_number = np.random.uniform(0,1)\n",
        "        if random_number <= epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.q_net.choose_action(state)\n",
        "        return action\n",
        "\n",
        "    def fill_memory(self):\n",
        "        state = self.env.reset()\n",
        "\n",
        "        # Loop min_replay_size times and append experience to memory\n",
        "        for _ in range(self.memory.min_replay_size):\n",
        "\n",
        "            # Randomly taking action\n",
        "            action = self.env.action_space.sample()\n",
        "\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            experience = (state, action, reward, done, next_state)\n",
        "\n",
        "            # Add to memory\n",
        "            self.memory.add(experience)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "\n",
        "    def training(self):\n",
        "        # Fill memory\n",
        "        self.fill_memory()\n",
        "        reward_buffer = deque(maxlen=100) # Rewards of the previous 100 episodes\n",
        "\n",
        "        reward_per_episode = 0.0\n",
        "        state = self.env.reset()\n",
        "        all_rewards = []\n",
        "\n",
        "        for step in range(self.max_num_steps):\n",
        "            # Computer epsilon\n",
        "            epsilon = np.interp(step, [0, self.epsilon_decay_intervals], [self.max_epsilon, self.min_epsilon])\n",
        "            # Choose action to take\n",
        "            action = self.choose_action(epsilon, state)\n",
        "\n",
        "            # Take action and add experience to memory\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            experience = (state, action, reward, done, next_state)\n",
        "            # Add to memory\n",
        "            self.memory.add(experience)\n",
        "\n",
        "            reward_per_episode += reward\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            # If done, 1 episode is done\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                reward_buffer.append(reward_per_episode)\n",
        "                all_rewards.append((step, reward_per_episode))\n",
        "                reward_per_episode = 0.0\n",
        "\n",
        "            # Get batch from memory for training\n",
        "            states, actions, rewards, dones, next_states = self.memory.get_batch(self.batch_size)\n",
        "\n",
        "            # Predict Q value with Q network\n",
        "            q_values = self.q_net(states)\n",
        "            action_q_values = torch.gather(input=q_values, dim=1, index=actions)\n",
        "\n",
        "            # Predict target with Target network\n",
        "            # Compute targets using the formulation sample = r + gamma * max q(s',a')\n",
        "            target_q_values = self.target_net(next_states)\n",
        "            max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "            targets = rewards + self.gamma * (1 - dones) * max_target_q_values\n",
        "\n",
        "            loss = torch.nn.functional.mse_loss(action_q_values, targets)\n",
        "\n",
        "            # Gradient descent for q-network\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Update target network\n",
        "            if (step + 1) % self.target_update_frequency == 0:\n",
        "                self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "            # Print training results\n",
        "            if (step + 1) % 1000 == 0:\n",
        "                average_reward = np.mean(reward_buffer)\n",
        "                print(f'Episode: {len(all_rewards)} Step: {step+1} Average reward: {average_reward}')\n",
        "        return all_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB0ZRTihvREN"
      },
      "outputs": [],
      "source": [
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "max_num_steps = 500000\n",
        "epsilon_decay_intervals = 150000\n",
        "gamma = 0.99 # discount\n",
        "alpha = 5e-4 # learning rate\n",
        "\n",
        "memory_size = 50000\n",
        "min_replay_size = 1000\n",
        "batch_size = 32\n",
        "\n",
        "target_update_frequency = 2000 # target network update frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2M07vUo7BKU"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"ALE/KungFuMaster-v5\")\n",
        "model = Agent(env, max_epsilon, min_epsilon, max_num_steps, \\\n",
        "                 epsilon_decay_intervals, gamma, alpha, \\\n",
        "                 memory_size, min_replay_size, batch_size, \\\n",
        "                 target_update_frequency)\n",
        "all_rewards = model.training()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.save('DQN_all_rewards_500k', all_rewards)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-08T14:24:44.202100Z",
          "iopub.execute_input": "2023-07-08T14:24:44.202401Z",
          "iopub.status.idle": "2023-07-08T14:24:44.211444Z",
          "shell.execute_reply.started": "2023-07-08T14:24:44.202376Z",
          "shell.execute_reply": "2023-07-08T14:24:44.210471Z"
        },
        "trusted": true,
        "id": "NNzZbb9atD0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'DQN_all_rewards_500k.npy')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-08T14:24:48.905967Z",
          "iopub.execute_input": "2023-07-08T14:24:48.906353Z",
          "iopub.status.idle": "2023-07-08T14:24:48.913151Z",
          "shell.execute_reply.started": "2023-07-08T14:24:48.906322Z",
          "shell.execute_reply": "2023-07-08T14:24:48.912122Z"
        },
        "trusted": true,
        "id": "QwA1y-s5tD0l",
        "outputId": "67f5d906-0d29-46c8-cb13-1acbb2259b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "/kaggle/working/ddqn_500.npy",
            "text/html": "<a href='ddqn_500.npy' target='_blank'>ddqn_500.npy</a><br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.q_net.state_dict(), \"./DQN_all_rewards_500k\")"
      ],
      "metadata": {
        "id": "OC_H9zdWtg9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}